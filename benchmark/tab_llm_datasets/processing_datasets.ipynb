{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# WikiTableQuestions",
   "id": "f5b5f7839be0afc9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Uploading Dataset to huggingface\n",
   "id": "11ac30ba160a59da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "api.upload_large_folder(\n",
    "    folder_path=\"wiki_table_questions/\",\n",
    "    repo_id=\"TableSenseAI/WikiTableQuestions\",\n",
    "    repo_type=\"dataset\",\n",
    ")\n"
   ],
   "id": "bfa63de482d0daba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Preprocessing Dataset",
   "id": "15d9aaec837d3a97"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "\n",
    "input_tsv = 'WikiTableQuestions-data/data/pristine-unseen-tables.tsv'         # Replace with your input file\n",
    "output_json = 'examples-test.json'\n",
    "\n",
    "data = []\n",
    "\n",
    "with open(input_tsv, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f, delimiter='\\t')\n",
    "    for idx, row in enumerate(reader):\n",
    "        base_path, _ = os.path.splitext(row[\"context\"])\n",
    "        entry = {\n",
    "            \"id\": f\"nu-{idx}\",\n",
    "            \"utterance\": row[\"utterance\"],\n",
    "            \"target_value\": row[\"targetValue\"],\n",
    "            \"context\": {\n",
    "                \"csv\": base_path + \".csv\",\n",
    "                \"html\": base_path + \".html\",\n",
    "                \"tsv\": base_path + \".tsv\"\n",
    "            }\n",
    "        }\n",
    "        data.append(entry)\n",
    "\n",
    "with open(output_json, 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, indent=2)\n",
    "\n",
    "print(f\"Wrote {len(data)} entries to {output_json}\")\n"
   ],
   "id": "5c09538a061137bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# FreeformTableQA",
   "id": "194105454225f7ec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Preprocessing Dataset",
   "id": "fd409e486f52565a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T09:54:03.954574Z",
     "start_time": "2025-06-23T09:54:03.273612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "input_dev = 'freeform-table-qa/raw-data/fetaQA-v1_test.jsonl'\n",
    "output_json = 'freeform-table-qa/examples/examples-test.json'\n",
    "\n",
    "csv_dir = 'freeform-table-qa/examples/tables/csv'\n",
    "tsv_dir = 'freeform-table-qa/examples/tables/tsv'\n",
    "html_dir = 'freeform-table-qa/examples/tables/html'\n",
    "# dict_keys(['feta_id', 'table_source_json', 'page_wikipedia_url', 'table_page_title', 'table_section_title', 'table_array', 'highlighted_cell_ids', 'question', 'answer'])\n",
    "data = []\n",
    "with open(input_dev, 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "\n",
    "result = []\n",
    "for item in data:\n",
    "    example_id = f\"feta_{item[\"feta_id\"]}\"\n",
    "\n",
    "    # Define output paths relative to examples/\n",
    "    csv_path = f\"examples/tables/csv-test/{example_id}.csv\"\n",
    "    tsv_path = f\"examples/tables/tsv-test/{example_id}.tsv\"\n",
    "    html_path = f\"examples/tables/html-test/{example_id}.html\"\n",
    "\n",
    "    # Absolute paths for writing files\n",
    "    csv_full = os.path.join('freeform-table-qa', csv_path)\n",
    "    tsv_full = os.path.join('freeform-table-qa', tsv_path)\n",
    "    html_full = os.path.join('freeform-table-qa', html_path)\n",
    "\n",
    "    # Write CSV\n",
    "    with open(csv_full, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(item[\"table_array\"])\n",
    "\n",
    "    # Write TSV\n",
    "    with open(tsv_full, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f, delimiter='\\t')\n",
    "        writer.writerows(item[\"table_array\"])\n",
    "\n",
    "    # Write HTML\n",
    "    with open(html_full, 'w', encoding='utf-8') as f:\n",
    "        f.write('<table border=\"1\">\\n')\n",
    "        for row in item[\"table_array\"]:\n",
    "            f.write('  <tr>' + ''.join(f'<td>{cell}</td>' for cell in row) + '</tr>\\n')\n",
    "        f.write('</table>\\n')\n",
    "\n",
    "    new_item = {\n",
    "        \"id\": example_id,\n",
    "        \"utterance\": item[\"question\"],\n",
    "        \"target_value\": item[\"answer\"],\n",
    "        \"context\": {\n",
    "            \"csv\": csv_path,\n",
    "            \"html\": tsv_path,\n",
    "            \"tsv\": html_path\n",
    "        }\n",
    "    }\n",
    "    result.append(new_item)\n",
    "\n",
    "# To save to a file:\n",
    "with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n"
   ],
   "id": "4d5c68f349864dd7",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Uploading Dataset to huggingface",
   "id": "e1b831fe634de1ea"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-06-23T09:55:44.463379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from huggingface_hub import HfApi, login\n",
    "login(token=\"\")\n",
    "api = HfApi()\n",
    "api.upload_large_folder(\n",
    "    folder_path=\"freeform-table-qa/\",\n",
    "    repo_id=\"TableSenseAI/FreeformTableQA\",\n",
    "    repo_type=\"dataset\",\n",
    ")"
   ],
   "id": "e1d8b7b3d1ed18cd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielbogdan/PycharmProjects/TableSense-AI/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Recovering from metadata files: 100%|██████████| 30998/30998 [00:04<00:00, 6423.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "---------- 2025-06-23 11:55:53 (0:00:00) ----------\n",
      "Files:   hashed 8/30998 (20.1M/66.2M) | pre-uploaded: 1/1 (14.3M/66.2M) (+30995 unsure) | committed: 3/30998 (19.6M/66.2M) | ignored: 0\n",
      "Workers: hashing: 9 | get upload mode: 1 | pre-uploading: 0 | committing: 0 | waiting: 0\n",
      "---------------------------------------------------\n",
      "\u001B[K\u001B[F\n",
      "---------- 2025-06-23 11:56:53 (0:01:00) ----------\n",
      "Files:   hashed 30998/30998 (66.2M/66.2M) | pre-uploaded: 1/1 (14.3M/66.2M) | committed: 678/30998 (25.0M/66.2M) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 0 | committing: 1 | waiting: 9\n",
      "---------------------------------------------------\n",
      "\u001B[K\u001B[F                       \n",
      "---------- 2025-06-23 11:57:53 (0:02:00) ----------\n",
      "Files:   hashed 30998/30998 (66.2M/66.2M) | pre-uploaded: 1/1 (14.3M/66.2M) | committed: 2553/30998 (26.7M/66.2M) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 0 | committing: 1 | waiting: 9\n",
      "---------------------------------------------------\n",
      "\u001B[K\u001B[F                       \n",
      "---------- 2025-06-23 11:58:54 (0:03:00) ----------\n",
      "Files:   hashed 30998/30998 (66.2M/66.2M) | pre-uploaded: 1/1 (14.3M/66.2M) | committed: 4203/30998 (29.4M/66.2M) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 0 | committing: 1 | waiting: 9\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
