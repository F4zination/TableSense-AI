{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"source\": \"# WikiTableQuestions\",\n",
    "   \"id\": \"f5b5f7839be0afc9\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"source\": \"### Uploading Dataset to huggingface\\n\",\n",
    "   \"id\": \"11ac30ba160a59da\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null,\n",
    "   \"source\": [\n",
    "    \"import os\\n\",\n",
    "    \"\\n\",\n",
    "    \"from huggingface_hub import HfApi\\n\",\n",
    "    \"\\n\",\n",
    "    \"api = HfApi()\\n\",\n",
    "    \"api.upload_large_folder(\\n\",\n",
    "    \"    folder_path=\\\"wiki_table_questions/\\\",\\n\",\n",
    "    \"    repo_id=\\\"TableSenseAI/WikiTableQuestions\\\",\\n\",\n",
    "    \"    repo_type=\\\"dataset\\\",\\n\",\n",
    "    \")\\n\"\n",
    "   ],\n",
    "   \"id\": \"bfa63de482d0daba\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"source\": \"### Preprocessing Dataset\",\n",
    "   \"id\": \"15d9aaec837d3a97\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null,\n",
    "   \"source\": [\n",
    "    \"import csv\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"\\n\",\n",
    "    \"input_tsv = 'WikiTableQuestions-data/data/pristine-unseen-tables.tsv'         # Replace with your input file\\n\",\n",
    "    \"output_json = 'examples-test.json'\\n\",\n",
    "    \"\\n\",\n",
    "    \"data = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"with open(input_tsv, 'r', encoding='utf-8') as f:\\n\",\n",
    "    \"    reader = csv.DictReader(f, delimiter='\\\\t')\\n\",\n",
    "    \"    for idx, row in enumerate(reader):\\n\",\n",
    "    \"        base_path, _ = os.path.splitext(row[\\\"context\\\"])\\n\",\n",
    "    \"        entry = {\\n\",\n",
    "    \"            \\\"id\\\": f\\\"nu-{idx}\\\",\\n\",\n",
    "    \"            \\\"utterance\\\": row[\\\"utterance\\\"],\\n\",\n",
    "    \"            \\\"target_value\\\": row[\\\"targetValue\\\"],\\n\",\n",
    "    \"            \\\"context\\\": {\\n\",\n",
    "    \"                \\\"csv\\\": base_path + \\\".csv\\\",\\n\",\n",
    "    \"                \\\"html\\\": base_path + \\\".html\\\",\\n\",\n",
    "    \"                \\\"tsv\\\": base_path + \\\".tsv\\\"\\n\",\n",
    "    \"            }\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"        data.append(entry)\\n\",\n",
    "    \"\\n\",\n",
    "    \"with open(output_json, 'w', encoding='utf-8') as f:\\n\",\n",
    "    \"    json.dump(data, f, indent=2)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Wrote {len(data)} entries to {output_json}\\\")\\n\"\n",
    "   ],\n",
    "   \"id\": \"5c09538a061137bd\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"source\": \"# FreeformTableQA\",\n",
    "   \"id\": \"194105454225f7ec\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"source\": \"### Preprocessing Dataset\",\n",
    "   \"id\": \"fd409e486f52565a\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-06-23T09:54:03.954574Z\",\n",
    "     \"start_time\": \"2025-06-23T09:54:03.273612Z\"\n",
    "    }\n",
    "   },\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"import json\\n\",\n",
    "    \"import csv\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"input_dev = 'freeform-table-qa/raw-data/fetaQA-v1_test.jsonl'\\n\",\n",
    "    \"output_json = 'freeform-table-qa/examples/examples-test.json'\\n\",\n",
    "    \"\\n\",\n",
    "    \"csv_dir = 'freeform-table-qa/examples/tables/csv'\\n\",\n",
    "    \"tsv_dir = 'freeform-table-qa/examples/tables/tsv'\\n\",\n",
    "    \"html_dir = 'freeform-table-qa/examples/tables/html'\\n\",\n",
    "    \"# dict_keys(['feta_id', 'table_source_json', 'page_wikipedia_url', 'table_page_title', 'table_section_title', 'table_array', 'highlighted_cell_ids', 'question', 'answer'])\\n\",\n",
    "    \"data = []\\n\",\n",
    "    \"with open(input_dev, 'r') as f:\\n\",\n",
    "    \"    for line in f:\\n\",\n",
    "    \"        data.append(json.loads(line))\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"result = []\\n\",\n",
    "    \"for item in data:\\n\",\n",
    "    \"    example_id = f\\\"feta_{item[\\\"feta_id\\\"]}\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Define output paths relative to examples/\\n\",\n",
    "    \"    csv_path = f\\\"examples/tables/csv-test/{example_id}.csv\\\"\\n\",\n",
    "    \"    tsv_path = f\\\"examples/tables/tsv-test/{example_id}.tsv\\\"\\n\",\n",
    "    \"    html_path = f\\\"examples/tables/html-test/{example_id}.html\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Absolute paths for writing files\\n\",\n",
    "    \"    csv_full = os.path.join('freeform-table-qa', csv_path)\\n\",\n",
    "    \"    tsv_full = os.path.join('freeform-table-qa', tsv_path)\\n\",\n",
    "    \"    html_full = os.path.join('freeform-table-qa', html_path)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Write CSV\\n\",\n",
    "    \"    with open(csv_full, 'w', newline='', encoding='utf-8') as f:\\n\",\n",
    "    \"        writer = csv.writer(f)\\n\",\n",
    "    \"        writer.writerows(item[\\\"table_array\\\"])\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Write TSV\\n\",\n",
    "    \"    with open(tsv_full, 'w', newline='', encoding='utf-8') as f:\\n\",\n",
    "    \"        writer = csv.writer(f, delimiter='\\\\t')\\n\",\n",
    "    \"        writer.writerows(item[\\\"table_array\\\"])\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Write HTML\\n\",\n",
    "    \"    with open(html_full, 'w', encoding='utf-8') as f:\\n\",\n",
    "    \"        f.write('<table border=\\\"1\\\">\\\\n')\\n\",\n",
    "    \"        for row in item[\\\"table_array\\\"]:\\n\",\n",
    "    \"            f.write('  <tr>' + ''.join(f'<td>{cell}</td>' for cell in row) + '</tr>\\\\n')\\n\",\n",
    "    \"        f.write('</table>\\\\n')\\n\",\n",
    "    \"\\n\",\n",
    "    \"    new_item = {\\n\",\n",
    "    \"        \\\"id\\\": example_id,\\n\",\n",
    "    \"        \\\"utterance\\\": item[\\\"question\\\"],\\n\",\n",
    "    \"        \\\"target_value\\\": item[\\\"answer\\\"],\\n\",\n",
    "    \"        \\\"context\\\": {\\n\",\n",
    "    \"            \\\"csv\\\": csv_path,\\n\",\n",
    "    \"            \\\"html\\\": tsv_path,\\n\",\n",
    "    \"            \\\"tsv\\\": html_path\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    result.append(new_item)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# To save to a file:\\n\",\n",
    "    \"with open(output_json, \\\"w\\\", encoding=\\\"utf-8\\\") as f:\\n\",\n",
    "    \"    json.dump(result, f, indent=2, ensure_ascii=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\"\n",
    "   ],\n",
    "   \"id\": \"4d5c68f349864dd7\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": 3\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"source\": \"### Uploading Dataset to huggingface\",\n",
    "   \"id\": \"e1b831fe634de1ea\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {\n",
    "    \"jupyter\": {\n",
    "     \"is_executing\": true\n",
    "    },\n",
    "    \"ExecuteTime\": {\n",
    "     \"start_time\": \"2025-06-23T09:55:44.463379Z\"\n",
    "    }\n",
    "   },\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"from huggingface_hub import HfApi, login\\n\",\n",
    "    \"login(token=\\\"\\\")\\n\",\n",
    "    \"api = HfApi()\\n\",\n",
    "    \"api.upload_large_folder(\\n\",\n",
    "    \"    folder_path=\\\"freeform-table-qa/\\\",\\n\",\n",
    "    \"    repo_id=\\\"TableSenseAI/FreeformTableQA\\\",\\n\",\n",
    "    \"    repo_type=\\\"dataset\\\",\\n\",\n",
    "    \")\"\n",
    "   ],\n",
    "   \"id\": \"e1d8b7b3d1ed18cd\",\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"/Users/danielbogdan/PycharmProjects/TableSense-AI/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\\n\",\n",
    "      \"  from .autonotebook import tqdm as notebook_tqdm\\n\",\n",
    "      \"Recovering from metadata files: 100%|██████████| 30998/30998 [00:04<00:00, 6423.39it/s]\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"\\n\",\n",
    "      \"\\n\",\n",
    "      \"\\n\",\n",
    "      \"---------- 2025-06-23 11:55:53 (0:00:00) ----------\\n\",\n",
    "      \"Files:   hashed 8/30998 (20.1M/66.2M) | pre-uploaded: 1/1 (14.3M/66.2M) (+30995 unsure) | committed: 3/30998 (19.6M/66.2M) | ignored: 0\\n\",\n",
    "      \"Workers: hashing: 9 | get upload mode: 1 | pre-uploading: 0 | committing: 0 | waiting: 0\\n\",\n",
    "      \"---------------------------------------------------\\n\",\n",
    "      \"\\u001B[K\\u001B[F\\n\",\n",
    "      \"---------- 2025-06-23 11:56:53 (0:01:00) ----------\\n\",\n",
    "      \"Files:   hashed 30998/30998 (66.2M/66.2M) | pre-uploaded: 1/1 (14.3M/66.2M) | committed: 678/30998 (25.0M/66.2M) | ignored: 0\\n\",\n",
    "      \"Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 0 | committing: 1 | waiting: 9\\n\",\n",
    "      \"---------------------------------------------------\\n\",\n",
    "      \"\\u001B[K\\u001B[F                       \\n\",\n",
    "      \"---------- 2025-06-23 11:57:53 (0:02:00) ----------\\n\",\n",
    "      \"Files:   hashed 30998/30998 (66.2M/66.2M) | pre-uploaded: 1/1 (14.3M/66.2M) | committed: 2553/30998 (26.7M/66.2M) | ignored: 0\\n\",\n",
    "      \"Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 0 | committing: 1 | waiting: 9\\n\",\n",
    "      \"---------------------------------------------------\\n\",\n",
    "      \"\\u001B[K\\u001B[F                       \\n\",\n",
    "      \"---------- 2025-06-23 11:58:54 (0:03:00) ----------\\n\",\n",
    "      \"Files:   hashed 30998/30998 (66.2M/66.2M) | pre-uploaded: 1/1 (14.3M/66.2M) | committed: 4203/30998 (29.4M/66.2M) | ignored: 0\\n\",\n",
    "      \"Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 0 | committing: 1 | waiting: 9\\n\",\n",
    "      \"---------------------------------------------------\\n\",\n",
    "      \"                             \"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"execution_count\": null\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {},\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}\n"
   ],
   "id": "ef22a9531fe22272"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
